{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19834,
     "status": "ok",
     "timestamp": 1756571409617,
     "user": {
      "displayName": "Daniel Ruderman",
      "userId": "14047671416305053483"
     },
     "user_tz": -180
    },
    "id": "2va9HDv46FrP",
    "outputId": "647ff5e4-242e-44a6-8e64-ec895e12006e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
      "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Minimal dependencies for the cleaning pipeline\n",
    "!pip -q install pandas==2.3.2 numpy==2.3.2 tqdm==4.67.1 nltk==3.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21573,
     "status": "ok",
     "timestamp": 1756570669588,
     "user": {
      "displayName": "Daniel Ruderman",
      "userId": "14047671416305053483"
     },
     "user_tz": -180
    },
    "id": "_ldzk-tP63qY",
    "outputId": "c66a9b9b-2026-46f8-e565-3ae4bbfbae75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# --------------- Hugging Face token ---------------\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_TOKEN_HERE\"\n",
    "login(os.environ[\"HF_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4879,
     "status": "ok",
     "timestamp": 1756570676054,
     "user": {
      "displayName": "Daniel Ruderman",
      "userId": "14047671416305053483"
     },
     "user_tz": -180
    },
    "id": "9PSRbr5q63nv",
    "outputId": "03d26f0e-ce8a-414c-da3e-bbf8239d38cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ---------- Your project root that contains `data/` ----------\n",
    "BASE_PATH = r\"/content/drive/My Drive/associations-ANLP\"\n",
    "\n",
    "# Convenience join\n",
    "def P(*parts):\n",
    "    return os.path.join(BASE_PATH, *parts)\n",
    "\n",
    "# Ensure output folder exists (no summary_tables at all)\n",
    "os.makedirs(P(\"data\", \"intermediate_preprocess_dataset_using_LWOW_code\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY8ghCiB63lb"
   },
   "outputs": [],
   "source": [
    "# ========== Only the functions actually used by the cleaning pipeline ==========\n",
    "\n",
    "def loadSimplifiedSWOW():\n",
    "    df_orig = pd.read_csv(P('data', 'original_dataset', 'SWOW-EN.R100.csv'))\n",
    "    countries = pd.read_csv(P('data', 'mapping_tables', 'country.csv'))\n",
    "\n",
    "    df = df_orig.copy()\n",
    "    # Gender\n",
    "    df = df.replace({'gender': {'Fe': 'Female', 'Ma': 'Male', 'X': 'Unknown'}})\n",
    "    # Education\n",
    "    df.education = df.education.fillna('Unknown')\n",
    "    df = df.replace({'education': {\n",
    "        1.0: 'No education',\n",
    "        2.0: 'Elementary school',\n",
    "        3.0: 'High school',\n",
    "        4.0: 'Bachelor degree',\n",
    "        5.0: 'Master degree'\n",
    "    }})\n",
    "    # Native Language\n",
    "    eng = ['Australia','Canada','Ireland','New Zealand','United Kingdom','United States','Other_English']\n",
    "    df['nativeLanguage'] = np.where(df['nativeLanguage'].isin(eng), 'English', 'Not English')\n",
    "    # Country\n",
    "    df['country'] = np.where(df['country'].isin(countries.value.values), df['country'], 'Unknown')\n",
    "    # Cue (convert to strings)\n",
    "    df['cue'] = [str(x) for x in df.cue.values]\n",
    "    # Responses (convert NANs to blanks)\n",
    "    df.R1 = df.R1.fillna('')\n",
    "    df.R2 = df.R2.fillna('')\n",
    "    df.R3 = df.R3.fillna('')\n",
    "    # Keep only needed vars\n",
    "    df = df[['age','gender','nativeLanguage','country','education','cue','R1','R2','R3']]\n",
    "    # Sort (kept for parity)\n",
    "    df_sorted = df.sort_values(by=['cue','age','gender','nativeLanguage','country','education'])\n",
    "    return df_sorted\n",
    "\n",
    "def loadTextFile(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    out = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if '\\t' in line:\n",
    "            line = line.split('\\t')\n",
    "        out.append(line)\n",
    "    return out\n",
    "\n",
    "# ---------- Cleaning stack ----------\n",
    "def cue100(df1, unqCues):\n",
    "    random.seed(30)\n",
    "    df1 = df1[df1['cue'].isin(list(unqCues))]  # Only keep cues in original\n",
    "    c = list(df1['cue'])\n",
    "    cCount = collections.Counter(c)\n",
    "    over100, under100 = {}, {}\n",
    "    for key, value in cCount.items():\n",
    "        if value > 100:\n",
    "            over100[key] = value\n",
    "        if value < 100:\n",
    "            under100[key] = value\n",
    "\n",
    "    df = df1.copy()\n",
    "    # Remove rows for cues that appear more than 100 times\n",
    "    if len(over100) > 0:\n",
    "        rows_to_remove = []\n",
    "        for c, count in over100.items():\n",
    "            dfCue = df1[df1['cue'] == c]\n",
    "            surplus = count - 100\n",
    "            rows_to_remove.append(random.sample(dfCue.index.tolist(), surplus))\n",
    "        rows_to_remove = [idx for group in rows_to_remove for idx in group]\n",
    "        df = df.drop(rows_to_remove)\n",
    "\n",
    "    # Add rows for cues that appear less than 100 times\n",
    "    if len(under100) > 0:\n",
    "        for c, count in under100.items():\n",
    "            deficit = 100 - count\n",
    "            rows_to_add = pd.DataFrame(\n",
    "                zip([c]*deficit, ['']*deficit, ['']*deficit, ['']*deficit),\n",
    "                columns=['cue','R1','R2','R3']\n",
    "            )\n",
    "            df = pd.concat([df, rows_to_add], ignore_index=True)\n",
    "\n",
    "    # Add rows for cues completely missing from the dataset\n",
    "    missingCues = list(set(unqCues) - set(df['cue']))\n",
    "    if len(missingCues) > 0:\n",
    "        for c in missingCues:\n",
    "            rows_to_add = pd.DataFrame(\n",
    "                zip([c]*100, ['']*100, ['']*100, ['']*100),\n",
    "                columns=['cue','R1','R2','R3']\n",
    "            )\n",
    "            df = pd.concat([df, rows_to_add], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def NA2Blank(df1):\n",
    "    df = df1.copy()\n",
    "    for col in ['cue','R1','R2','R3']:\n",
    "        df[col] = [x if isinstance(x, str) else '' for x in df[col]]\n",
    "    return df\n",
    "\n",
    "def Lowercase(df1):\n",
    "    df = df1.copy()\n",
    "    for col in ['cue','R1','R2','R3']:\n",
    "        df[col] = [x.lower() for x in df[col]]\n",
    "    return df\n",
    "\n",
    "def RemoveUnderscore(df1):\n",
    "    df = df1.copy()\n",
    "    for col in ['cue','R1','R2','R3']:\n",
    "        df[col] = [x.replace('_',' ') for x in df[col]]\n",
    "    return df\n",
    "\n",
    "def RemoveRespArticles(df1, unqCues):\n",
    "    df = df1.copy()\n",
    "    for col in ['R1','R2','R3']:\n",
    "        for prefix in ['a ','an ','the ','to ']:\n",
    "            mask = (df[col].str.startswith(prefix)) & (~df[col].isin(unqCues))\n",
    "            df.loc[mask, col] = df.loc[mask, col].str[len(prefix):]\n",
    "    return df\n",
    "\n",
    "def AddSpaceOrHyphen(df1, missingDict):\n",
    "    df = df1.copy()\n",
    "    for col in ['cue','R1','R2','R3']:\n",
    "        df[col] = df[col].map(missingDict).fillna(df[col])\n",
    "    return df\n",
    "\n",
    "def Spelling(df1, spelling_dict):\n",
    "    df = df1.copy()\n",
    "    for col in ['cue','R1','R2','R3']:\n",
    "        df[col] = df[col].map(spelling_dict).fillna(df[col])\n",
    "    return df\n",
    "\n",
    "def Lemmatization(df1):\n",
    "    df = df1.copy()\n",
    "    for col in ['cue','R1','R2','R3']:\n",
    "        df[col] = [lemmatizer.lemmatize(x) for x in df[col]]\n",
    "        df[col] = [x.replace('men','man') for x in df[col]]\n",
    "        df[col] = [x.replace('hands','hand') for x in df[col]]\n",
    "    return df\n",
    "\n",
    "def RemoveCueResp(df1):\n",
    "    df = df1.copy()\n",
    "    for col in ['R1','R2','R3']:\n",
    "        df[col] = np.where(df[col] == df['cue'], '', df[col])\n",
    "    return df\n",
    "\n",
    "def RemoveDupeResp(df1):\n",
    "    df = df1.copy()\n",
    "    df['R3'] = np.where((df['R3'] == df['R1']) | (df['R3'] == df['R2']), '', df['R3'])\n",
    "    df['R2'] = np.where(df['R2'] == df['R1'], '', df['R2'])\n",
    "    return df\n",
    "\n",
    "def ShiftResp(df1):\n",
    "    df = df1.copy()\n",
    "    # _ _ X -> X _ _\n",
    "    df['R1'] = np.where((df['R1']== '') & (df['R2']== '') & (df['R3']!=''), df['R3'], df['R1'])\n",
    "    df['R3'] = np.where(df['R1'] == df['R3'], '', df['R3'])\n",
    "    # _ X _ -> X _ _\n",
    "    df['R1'] = np.where((df['R1']== '') & (df['R2']!='') & (df['R3']==''), df['R2'], df['R1'])\n",
    "    df['R2'] = np.where(df['R1'] == df['R2'], '', df['R2'])\n",
    "    # _ X X -> X _ X\n",
    "    df['R1'] = np.where((df['R1']== '') & (df['R2']!='') & (df['R3']!=''), df['R2'], df['R1'])\n",
    "    df['R2'] = np.where(df['R1'] == df['R2'], '', df['R2'])\n",
    "    # X _ X -> X X _\n",
    "    df['R2'] = np.where((df['R1']!='') & (df['R2']== '') & (df['R3']!=''), df['R3'], df['R2'])\n",
    "    df['R3'] = np.where(df['R2'] == df['R3'], '', df['R3'])\n",
    "    return df\n",
    "\n",
    "def SortColumns(df1):\n",
    "    df = df1.copy()\n",
    "    df = df[['cue','R1','R2','R3']]\n",
    "    df = df.sort_values(by=['cue','R1','R2','R3'])\n",
    "    return df\n",
    "\n",
    "def cleaningPipeline(df1, unqCues, missingDict, spelling_dict, name):\n",
    "    df = df1.copy()\n",
    "    df = NA2Blank(df)\n",
    "    df = Lowercase(df)\n",
    "    df = RemoveUnderscore(df)\n",
    "    df = RemoveRespArticles(df, unqCues)\n",
    "    df = AddSpaceOrHyphen(df, missingDict)\n",
    "    df = Spelling(df, spelling_dict)\n",
    "    df = Lemmatization(df)\n",
    "    df = cue100(df, unqCues)\n",
    "    df = RemoveCueResp(df)\n",
    "    df = RemoveDupeResp(df)\n",
    "    df = ShiftResp(df)\n",
    "    df = SortColumns(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169714,
     "status": "ok",
     "timestamp": 1756570853184,
     "user": {
      "displayName": "Daniel Ruderman",
      "userId": "14047671416305053483"
     },
     "user_tz": -180
    },
    "id": "ywTogn3-63i5",
    "outputId": "7295fa6c-491c-486b-f1ec-fa7ed1cf6b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Humans-only cleaned CSV to:\n",
      "  /content/drive/MyDrive/ANLP_project_final/data/intermediate_preprocess_dataset_using_LWOW_code/FA_Humans.csv\n",
      "Cleaned shape: (1154600, 4)\n"
     ]
    }
   ],
   "source": [
    "# 1) Load spelling lookup\n",
    "spelling_path = P('data', 'intermediate_preprocess_dataset_using_LWOW_code', 'mapping_tables', 'EnglishCustomDict.txt')\n",
    "spelling = loadTextFile(spelling_path)\n",
    "spelling_dict = {a.lower(): b.lower() for [a, b] in spelling}\n",
    "\n",
    "# 2) Load SWOW (Humans only) and build cue set\n",
    "FA_SWOW = loadSimplifiedSWOW()\n",
    "FA_SWOW = FA_SWOW[FA_SWOW['cue'] != 'nan']  # keep literal 'nan' strings out\n",
    "\n",
    "OrigCues = FA_SWOW['cue']\n",
    "OrigCues = [x for x in OrigCues if isinstance(x, str)]      # remove non-strings\n",
    "OrigCues = [x.lower() for x in OrigCues]                    # lowercase\n",
    "OrigCues = [spelling_dict.get(x, x) for x in OrigCues]      # spelling normalize\n",
    "OrigCues = [lemmatizer.lemmatize(x) if x != 'men' else 'man' for x in OrigCues]  # lemmatize + men→man\n",
    "unqCues = list(set(OrigCues))\n",
    "\n",
    "# 3) Build missingDict (spaces/hyphens) from WordNet\n",
    "wnWords = []\n",
    "for syn in wn.all_synsets():\n",
    "    wnWords.append([str(lemma.name()) for lemma in syn.lemmas()])\n",
    "wnWordsFlat = [item for sub in wnWords for item in sub]\n",
    "wnWordsLower = [x.lower() for x in wnWordsFlat]\n",
    "\n",
    "noSpacesDict   = {x.replace('_',''): x.replace('_',' ') for x in wnWordsLower}\n",
    "noHyphensDict  = {x.replace('-',''): x for x in wnWordsLower}\n",
    "onlyNoSpaces   = list(set(noSpacesDict.keys())  - set(wnWordsLower))\n",
    "onlyNoHyphens  = list(set(noHyphensDict.keys()) - set(wnWordsLower))\n",
    "onlyNoSpacesDict  = {x: noSpacesDict[x]  for x in onlyNoSpaces}\n",
    "onlyNoHyphensDict = {x: noHyphensDict[x] for x in onlyNoHyphens}\n",
    "missingDict = onlyNoSpacesDict.copy()\n",
    "missingDict.update(onlyNoHyphensDict)\n",
    "\n",
    "# 4) Clean Humans (SWOW) only and save CSV\n",
    "clean_humans = cleaningPipeline(FA_SWOW, unqCues, missingDict, spelling_dict, name='Humans')\n",
    "\n",
    "out_path = P('data', 'intermediate_preprocess_dataset_using_LWOW_code', 'FA_Humans.csv')\n",
    "clean_humans.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved Humans-only cleaned CSV to:\\n  {out_path}\")\n",
    "print(f\"Cleaned shape: {clean_humans.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpeYYXuN8GHY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InZqppji8GE0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPP3FSpkjns16EnU7xMjoiL",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
