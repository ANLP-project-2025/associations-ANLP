{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19912,
     "status": "ok",
     "timestamp": 1756562736904,
     "user": {
      "displayName": "Daniel Ruderman",
      "userId": "14047671416305053483"
     },
     "user_tz": -180
    },
    "id": "YxXu_LMQcUft",
    "outputId": "c2a3e24c-ac31-4a9e-9bd6-9212b79fba2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m137.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q \\\n",
    "    \"numpy==2.2.6\" \\\n",
    "    \"pandas==2.3.1\" \\\n",
    "    \"openpyxl==3.1.5\" \\\n",
    "    \"et-xmlfile==2.0.0\" \\\n",
    "    \"python-dateutil==2.9.0.post0\" \\\n",
    "    \"pytz==2025.2\" \\\n",
    "    \"tzdata==2025.2\" \\\n",
    "    \"six==1.17.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17458,
     "status": "ok",
     "timestamp": 1756562787952,
     "user": {
      "displayName": "Daniel Ruderman",
      "userId": "14047671416305053483"
     },
     "user_tz": -180
    },
    "id": "6KLbkQkEdVHX",
    "outputId": "3c2d2d8d-03dc-4031-860e-18c4814f1ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "# --------------- Hugging Face token ---------------\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_TOKEN_HERE\"\n",
    "login(os.environ[\"HF_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dZx4AuldVYK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def process(file_path: str, output_xlsx: str = None, seed: int = SEED) -> None:\n",
    "    # ---- Reproducibility ----\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # ---- Load CSV (keep defaults to mirror your PyCharm run) ----\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # ---- Lowercase cue and response columns ----\n",
    "    for col in [\"cue\", \"R1\", \"R2\", \"R3\"]:\n",
    "        # use .str only if the column exists; match your original intent\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.lower()\n",
    "\n",
    "    # ---- Original stats ----\n",
    "    original_row_count = len(df)\n",
    "    original_unique_cues = df[\"cue\"].nunique()\n",
    "\n",
    "    # ---- Drop rows with any missing R1/R2/R3 ----\n",
    "    cleaned_df = df.dropna(subset=[\"R1\", \"R2\", \"R3\"])\n",
    "    new_row_count = len(cleaned_df)\n",
    "    new_unique_cues = cleaned_df[\"cue\"].nunique()\n",
    "\n",
    "    # ---- Unique responses across R1–R3 ----\n",
    "    unique_responses = pd.unique(cleaned_df[[\"R1\", \"R2\", \"R3\"]].values.ravel())\n",
    "    num_unique_responses = len(unique_responses)\n",
    "\n",
    "    # ---- Stats on cue frequencies ----\n",
    "    cue_counts = cleaned_df[\"cue\"].value_counts()\n",
    "    min_per_cue = cue_counts.min()\n",
    "    max_per_cue = cue_counts.max()\n",
    "    mean_per_cue = cue_counts.mean()\n",
    "    median_per_cue = cue_counts.median()\n",
    "\n",
    "    # ---- Percentage of rows removed ----\n",
    "    percent_removed = 100 * (original_row_count - new_row_count) / original_row_count\n",
    "\n",
    "    # ---- Print main stats (kept your original print messages) ----\n",
    "    print(f\"Cleaned data saved to: {output_xlsx}\")\n",
    "    print(f\"Original number of different cues: {original_unique_cues}\")\n",
    "    print(f\"New number of different cues: {new_unique_cues}\")\n",
    "    print(f\"Original number of rows: {original_row_count}\")\n",
    "    print(f\"New number of rows: {new_row_count}\")\n",
    "    print(f\"Percentage of rows removed: {percent_removed:.2f}%\")\n",
    "    print(f\"Number of different responses (R1, R2, R3): {num_unique_responses}\")\n",
    "    print(f\"Rows per cue - min: {min_per_cue}, max: {max_per_cue}, mean: {mean_per_cue:.2f}, median: {median_per_cue}\")\n",
    "\n",
    "    print(\"\\nCues with at least N rows:\")\n",
    "    for threshold in range(10, 101, 10):\n",
    "        count = (cue_counts >= threshold).sum()\n",
    "        print(f\" ≥ {threshold}: {count} cues\")\n",
    "\n",
    "    # ---- Filter to cues with at least 80 rows ----\n",
    "    valid_cues = cue_counts[cue_counts >= 80].index\n",
    "    filtered_df = cleaned_df[cleaned_df[\"cue\"].isin(valid_cues)]\n",
    "\n",
    "    # ---- Sample exactly 80 rows per cue (deterministic) ----\n",
    "    sampled_df = (\n",
    "        filtered_df.groupby(\"cue\", group_keys=False)\n",
    "        .sample(n=80, random_state=seed)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    # Lowercase everything in the final table (like your original)\n",
    "    sampled_df = sampled_df.map(lambda x: str(x).lower() if not pd.isna(x) else x)\n",
    "\n",
    "    # ---- Save outputs ----\n",
    "    if output_xlsx is not None:\n",
    "        os.makedirs(os.path.dirname(output_xlsx), exist_ok=True)\n",
    "        sampled_df.to_excel(output_xlsx, index=False)  # openpyxl backend (pinned above)\n",
    "    print(f\"\\nDownsampled dataset (80 rows per cue) saved to: {output_xlsx}\")\n",
    "\n",
    "    # ---- Downsampled stats ----\n",
    "    downsampled_row_count = len(sampled_df)\n",
    "    downsampled_unique_cues = sampled_df[\"cue\"].nunique()\n",
    "    downsampled_unique_responses = pd.unique(sampled_df[[\"R1\", \"R2\", \"R3\"]].values.ravel())\n",
    "    num_downsampled_unique_responses = len(downsampled_unique_responses)\n",
    "    downsampled_cue_counts = sampled_df[\"cue\"].value_counts()\n",
    "    min_down = downsampled_cue_counts.min()\n",
    "    max_down = downsampled_cue_counts.max()\n",
    "    mean_down = downsampled_cue_counts.mean()\n",
    "    median_down = downsampled_cue_counts.median()\n",
    "\n",
    "    print(f\"Downsampled number of different cues: {downsampled_unique_cues}\")\n",
    "    print(f\"Downsampled number of rows: {downsampled_row_count}\")\n",
    "    print(f\"Number of different responses (R1, R2, R3): {num_downsampled_unique_responses}\")\n",
    "    print(f\"Rows per cue - min: {min_down}, max: {max_down}, mean: {mean_down:.2f}, median: {median_down}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61861,
     "status": "ok",
     "timestamp": 1756562859709,
     "user": {
      "displayName": "Daniel Ruderman",
      "userId": "14047671416305053483"
     },
     "user_tz": -180
    },
    "id": "rJl5lN9eeetP",
    "outputId": "ccefa9eb-b33b-4c59-e4cf-ef7d251690b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned data saved to: /content/drive/MyDrive/ANLP_project_final/data/final_processed_SWOW_data/cleaned_data_FA_Humans.xlsx\n",
      "Original number of different cues: 11546\n",
      "New number of different cues: 11393\n",
      "Original number of rows: 1154600\n",
      "New number of rows: 976223\n",
      "Percentage of rows removed: 15.45%\n",
      "Number of different responses (R1, R2, R3): 110485\n",
      "Rows per cue - min: 32, max: 100, mean: 85.69, median: 88.0\n",
      "\n",
      "📊 Cues with at least N rows:\n",
      " ≥ 10: 11393 cues\n",
      " ≥ 20: 11393 cues\n",
      " ≥ 30: 11393 cues\n",
      " ≥ 40: 11388 cues\n",
      " ≥ 50: 11368 cues\n",
      " ≥ 60: 11277 cues\n",
      " ≥ 70: 10834 cues\n",
      " ≥ 80: 8992 cues\n",
      " ≥ 90: 4468 cues\n",
      " ≥ 100: 13 cues\n",
      "\n",
      "✅ Downsampled dataset (80 rows per cue) saved to: /content/drive/MyDrive/ANLP_project_final/data/final_processed_SWOW_data/cleaned_data_FA_Humans.xlsx\n",
      "Downsampled number of different cues: 8992\n",
      "Downsampled number of rows: 719360\n",
      "Number of different responses (R1, R2, R3): 89815\n",
      "Rows per cue - min: 80, max: 80, mean: 80.00, median: 80.0\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = r\"/content/drive/My Drive/associations-ANLP\"\n",
    "\n",
    "INPUT_CSV   = os.path.join(BASE_PATH, r\"data/intermediate_preprocess_dataset_using_LWOW_code/FA_Humans.csv\")\n",
    "OUTPUT_XLSX = os.path.join(BASE_PATH, r\"data/final_processed_SWOW_data/cleaned_data_FA_Humans.xlsx\")\n",
    "\n",
    "process(INPUT_CSV, OUTPUT_XLSX)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOkOq3RxlljgT+vNyoF/Ca5",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
