{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZS9oYu43-zv"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 🔧 Setup: Install Packages\n",
    "# ===============================\n",
    "!pip install -q \\\n",
    "  \"transformers>=4.41,<5\" \\\n",
    "  \"datasets==2.19.1\" \\\n",
    "  \"peft==0.10.0\" \\\n",
    "  \"accelerate>=0.34.2\" \\\n",
    "  \"bitsandbytes>=0.43.3\" \\\n",
    "  \"scikit-learn\" \\\n",
    "  \"openpyxl\" \\\n",
    "  \"pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nycAJTqpMIKf"
   },
   "outputs": [],
   "source": [
    "import torch, sys, subprocess\n",
    "mm = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "triton_by_torch = {\"2.5\":\"3.2.0\",\"2.4\":\"3.0.0\",\"2.3\":\"2.3.1\",\"2.2\":\"2.2.0\"}\n",
    "target = triton_by_torch.get(mm, \"3.2.0\")\n",
    "print(f\"Torch {torch.__version__} → Installing Triton {target}\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"triton=={target}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8tfqh_s4DkC"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Import packages & login\n",
    "# ===============================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ===============================\n",
    "# Determinism preamble (FIRST cell, before importing torch)\n",
    "# ===============================\n",
    "import os\n",
    "\n",
    "# Required by PyTorch for cuBLAS determinism on CUDA >= 10.2\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"   # or \":16:8\" if you prefer\n",
    "\n",
    "# Optional: makes hashing stable across processes\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "import random, torch, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer, set_seed\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --------------- Hugging Face token ---------------\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_TOKEN_HERE\"\n",
    "login(os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# --------------- Reproducibility ---------------\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETMOylZjWq3E"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Generate 80× responses per cue on validation split\n",
    "#      • fine-tuned model\n",
    "#      • base model\n",
    "#      + save raw outputs, conversation history, and prompts\n",
    "#      + batched via num_return_sequences with OOM fallback\n",
    "# =========================================================\n",
    "\n",
    "import os, gc, re, time, math, random, hashlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# ---------- Reproducibility & deterministic sampling --------------\n",
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(GLOBAL_SEED)\n",
    "\n",
    "# Turn off TF32 to avoid numerics shifting between runs/GPUs\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# Deterministic/cuDNN settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Enforce deterministic algorithms (now safe because we set the env var above)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "# --------------------------------------------------------------------\n",
    "# change if you use another path\n",
    "BASE_PATH = r\"/content/drive/My Drive/associations-ANLP\"\n",
    "\n",
    "TEST_XLSX_PATH  = os.path.join(BASE_PATH, r\"data/final_processed_SWOW_data/test.xlsx\")\n",
    "\n",
    "# IMPORTANT: if you re-finetuned, update FINETUNED_MODEL to your current run's merged or adapter path\n",
    "FINETUNED_MODEL = os.path.join(BASE_PATH, r\"full_llama3_8b_system_prompt_lora_SFT_SWOW_tgt_qkvo_tr7194c_val899c_r16_a32_do0p1_lr0.0001_bs16_ga4/merged_model\")\n",
    "BASE_MODEL  = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "SAVE_DIR_FT   = os.path.join(BASE_PATH, r\"data/fine_tuned_and_base_models_comparisons/free_associations_task\")\n",
    "os.makedirs(SAVE_DIR_FT, exist_ok=True)\n",
    "\n",
    "SAVE_DIR_BASE = os.path.join(BASE_PATH, r\"data/fine_tuned_and_base_models_comparisons/free_associations_task\")\n",
    "os.makedirs(SAVE_DIR_BASE, exist_ok=True)\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# ---------- Precision / quant (use the SAME for both models) --------\n",
    "USE_4BIT = False  # set True if you prefer 4-bit generation for both\n",
    "\n",
    "def _bf16_supported():\n",
    "    return torch.cuda.is_available() and getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
    "\n",
    "if USE_4BIT:\n",
    "    load_kwargs = dict(\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if _bf16_supported() else torch.float16,\n",
    "        ),\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        # A100 → bf16; T4 → fp16\n",
    "        _dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n",
    "        load_kwargs = dict(device_map=\"auto\", torch_dtype=_dtype, use_safetensors=True)\n",
    "    else:\n",
    "        load_kwargs = dict(torch_dtype=torch.float32, use_safetensors=True)\n",
    "\n",
    "# ---------- Sampling knobs (unchanged except batching) ---------------\n",
    "NUM_SAMPLES_PER_CUE = 80\n",
    "TRY_AT_ONCE         = 80   # first attempt per call; auto-shrinks on OOM\n",
    "MAX_NEW_TOKENS      = 50\n",
    "TEMPERATURE         = 0.7\n",
    "TOP_P               = 0.9\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "Task:\n",
    " - You will be provided with an input word: write the first 3 words you associate to it separated by a comma.\n",
    " - No additional output text is allowed.\n",
    "\n",
    "Constraints:\n",
    " - no carriage return characters are allowed in the answers.\n",
    " - answers should be as short as possible.\n",
    "\n",
    "Example:\n",
    " Input: sea\n",
    " Output: water, beach, sun\"\"\"\n",
    "\n",
    "def build_prompt_text(tokenizer, cue: str) -> str:\n",
    "    \"\"\"Render messages with the official Llama-3 chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": str(cue).strip()},\n",
    "    ]\n",
    "    # This appends the assistant header so generation starts in assistant turn\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def build_history(cue: str, assistant_text: str) -> str:\n",
    "    \"\"\"Readable transcript (system → user → assistant).\"\"\"\n",
    "    return (\n",
    "        \"<system>\\n\"   + SYSTEM_PROMPT.strip() + \"\\n</system>\\n\"\n",
    "        \"<user>\\n\"     + str(cue).strip()      + \"\\n</user>\\n\"\n",
    "        \"<assistant>\\n\"+ (assistant_text or \"\").strip() + \"\\n</assistant>\"\n",
    "    )\n",
    "\n",
    "def extract_responses(text: str, n=3):\n",
    "    \"\"\"\n",
    "    Extract up to n words from a comma-separated generation.\n",
    "    We *don’t* expect 'Answer:' anymore. Normalize lightly.\n",
    "    \"\"\"\n",
    "    # Keep letters, digits, spaces, hyphens, and commas; lowercase\n",
    "    clean = re.sub(r\"[^\\w,\\- ]\", \"\", text.lower())\n",
    "    # robust split for \",\", \" ,\", \", \" etc.\n",
    "    words = [w.strip() for w in re.split(r\"\\s*,\\s*\", clean) if w.strip()]\n",
    "    return (words + [\"\", \"\", \"\"])[:n]\n",
    "\n",
    "def _per_cue_seed(cue_index: int) -> int:\n",
    "    # Stable per-cue seed independent of Python's hash randomization\n",
    "    return (GLOBAL_SEED * 1_000_003 + cue_index) & 0x7FFFFFFF\n",
    "\n",
    "def _generate_many_for_cue(model, tok, cue: str, cue_index: int) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate NUM_SAMPLES_PER_CUE samples for one cue using num_return_sequences,\n",
    "    with per-batch seeding that makes results reproducible and invariant to OOM chunking.\n",
    "    (No `generator=` kwarg is used, so it's compatible with more transformers builds.)\n",
    "    \"\"\"\n",
    "    prompt_text = build_prompt_text(tok, cue)\n",
    "    prompt_ids  = tok(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    recs = []\n",
    "    remaining   = NUM_SAMPLES_PER_CUE\n",
    "    try_at_once = TRY_AT_ONCE\n",
    "    produced    = 0\n",
    "\n",
    "    # Stable per-cue base seed\n",
    "    base_seed = (GLOBAL_SEED * 1_000_003 + cue_index) & 0x7FFFFFFF\n",
    "\n",
    "    while remaining > 0:\n",
    "        cur_n = min(try_at_once, remaining)\n",
    "\n",
    "        # ---- Repro trick: reseed RNG per *batch* based only on how many\n",
    "        # samples we've already produced for this cue. This makes outcomes\n",
    "        # independent of how we chunk (OOM backoff) and fully reproducible.\n",
    "        seed_this_batch = base_seed + produced\n",
    "        torch.manual_seed(seed_this_batch)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed_this_batch)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **prompt_ids,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    do_sample=True, temperature=TEMPERATURE, top_p=TOP_P,\n",
    "                    pad_token_id=tok.eos_token_id,\n",
    "                    eos_token_id=tok.eos_token_id,\n",
    "                    num_return_sequences=cur_n,\n",
    "                )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            try_at_once = max(1, try_at_once // 2)\n",
    "            if cur_n == 1 and try_at_once == 1:\n",
    "                raise\n",
    "            continue\n",
    "\n",
    "        prompt_len = prompt_ids[\"input_ids\"].shape[1]\n",
    "        for i in range(out.size(0)):\n",
    "            gen_only = out[i, prompt_len:]\n",
    "            raw = tok.decode(gen_only, skip_special_tokens=True)\n",
    "            r1, r2, r3 = extract_responses(raw)\n",
    "            recs.append(dict(\n",
    "                cue=cue,\n",
    "                R1=r1, R2=r2, R3=r3,\n",
    "                raw_output=raw,\n",
    "                history=build_history(cue, raw),\n",
    "                system_prompt=SYSTEM_PROMPT.strip(),\n",
    "                user_prompt=str(cue).strip(),\n",
    "            ))\n",
    "\n",
    "        produced  += cur_n\n",
    "        remaining -= cur_n\n",
    "\n",
    "    return recs\n",
    "\n",
    "def generate_dataset(model_id_or_path: str, tag: str):\n",
    "    \"\"\"Return list[dict]: {cue, R1, R2, R3, raw_output, history, system_prompt, user_prompt} for the val cues.\"\"\"\n",
    "    print(f\"\\n🔹 Loading {tag} model …\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id_or_path, use_fast=True)\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id_or_path, **load_kwargs)\n",
    "    model.eval()\n",
    "\n",
    "    # ---------- read validation cues from Excel ----------\n",
    "    val_df = pd.read_excel(VAL_XLSX_PATH)\n",
    "    cues = val_df[\"cue\"].dropna().astype(str).str.strip().unique()\n",
    "    recs = []\n",
    "\n",
    "    for idx, cue in enumerate(tqdm(cues, desc=f\"{tag} {NUM_SAMPLES_PER_CUE}× per cue\")):\n",
    "        recs.extend(_generate_many_for_cue(model, tok, cue, idx))\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # light hygiene between cues\n",
    "\n",
    "    # -------- cleanup to free VRAM --------\n",
    "    del model; del tok; gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return recs\n",
    "\n",
    "# ---------- Fine-tuned model -----------------------------------\n",
    "t0 = time.time()\n",
    "ft_rows = generate_dataset(FINETUNED_MODEL, \"Finetuned\")\n",
    "ft_tag  = os.path.basename(os.path.normpath(FINETUNED_MODEL))  # folder name like 'merged_model' or run dir\n",
    "ft_path = os.path.join(SAVE_DIR_FT, f\"full_val_ft_{ft_tag}_predictions.xlsx\")\n",
    "pd.DataFrame(ft_rows).to_excel(ft_path, index=False)  # ← Excel\n",
    "print(f\"Fine-tuned predictions saved → {ft_path} ({len(ft_rows):,} rows)\")\n",
    "print(f\"{time.time()-t0:.1f}s elapsed\")\n",
    "\n",
    "# ---------- Base model -----------------------------------------\n",
    "t1 = time.time()\n",
    "base_rows = generate_dataset(BASE_MODEL, \"Base\")\n",
    "base_path = os.path.join(SAVE_DIR_BASE, \"llama3_8b_full_val_base_predictions.xlsx\")\n",
    "pd.DataFrame(base_rows).to_excel(base_path, index=False)  # ← Excel\n",
    "print(f\"Base-model predictions saved → {base_path} ({len(base_rows):,} rows)\")\n",
    "print(f\"{time.time()-t1:.1f}s elapsed (second run)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIR0ueTJNx4D"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate fine-tuned (FT) vs. base LLaMA-3 model predictions against human\n",
    "word association data (SWOW val) on a cue-by-cue basis.\n",
    "\n",
    "This script:\n",
    "-----------\n",
    "1. Loads three datasets:\n",
    "    • Human associations (val.xlsx)\n",
    "    • Fine-tuned model predictions (80 runs per cue, .xlsx)\n",
    "    • Base model predictions (80 runs per cue, .xlsx)\n",
    "\n",
    "2. Normalizes all responses (lowercase, strip, keep only letters).\n",
    "\n",
    "3. Converts triplets (R1, R2, R3) into a \"long\" format for counting.\n",
    "\n",
    "4. Builds frequency distributions for each cue:\n",
    "    • Human distribution: counts of each unique word from humans\n",
    "    • FT model distribution: pooled counts over all runs\n",
    "    • Base model distribution: pooled counts over all runs\n",
    "\n",
    "5. For each cue, computes:\n",
    "    • Top-K overlap (K=10): Precision@K, Recall@K, F1@K, Jaccard@K\n",
    "    • Cosine similarity between full distributions\n",
    "    • Jensen–Shannon divergence (JSD) between full distributions\n",
    "    • Spearman rank correlation of shared vocabulary\n",
    "    • Shannon entropy of each distribution (diversity)\n",
    "    • Jaccard over ALL unique responses (set overlap, no frequency)\n",
    "\n",
    "6. Also computes row-level Hit@3:\n",
    "    • For each model output triplet, check if ≥1 word is in human set.\n",
    "\n",
    "7. Aggregates results:\n",
    "    • Macro averages (mean per cue) with bootstrap 95% CIs for FT and Base\n",
    "    • Δ (FT mean – Base mean) for each metric\n",
    "\n",
    "8. Outputs:\n",
    "    • Prints headline macro metrics for quick comparison (covers ALL paired metrics dynamically)\n",
    "    • Saves full per-cue metrics table to XLSX for deeper analysis\n",
    "    • Prints sample top-10 lists for first few cues for qualitative inspection\n",
    "\n",
    "Purpose:\n",
    "--------\n",
    "To quantitatively measure how closely each model’s associations match\n",
    "human associations, both in terms of exact word overlap and in distributional\n",
    "similarity, enabling a clear comparison between a fine-tuned and a base model.\n",
    "\"\"\"\n",
    "\n",
    "import os, re, math, json, numpy as np, pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# ---------- paths (Excel) ----------\n",
    "HUM_PATH  = os.path.join(BASE_PATH, r\"data/final_processed_SWOW_data/val.xlsx\")\n",
    "FT_PATH   = os.path.join(BASE_PATH, r\"data/fine_tuned_and_base_models_comparisons/free_associations_task/full_val_ft_merged_model_predictions.xlsx\")\n",
    "BASE_PATH = os.path.join(BASE_PATH, r\"data/fine_tuned_and_base_models_comparisons/free_associations_task/llama3_8b_full_val_base_predictions.xlsx\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _norm(w):\n",
    "    return re.sub(r\"[^a-z]\", \"\", str(w).strip().lower())\n",
    "\n",
    "def melt_triplets(df):\n",
    "    cols = [c for c in df.columns if c.lower() in {\"cue\",\"r1\",\"r2\",\"r3\"}]\n",
    "    df = df[cols].copy()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    for c in [\"cue\",\"r1\",\"r2\",\"r3\"]:\n",
    "        df[c] = df[c].map(_norm)\n",
    "    long = df.melt(id_vars=[\"cue\"], value_vars=[\"r1\",\"r2\",\"r3\"], value_name=\"resp\").dropna()\n",
    "    long = long[long[\"resp\"] != \"\"]\n",
    "    return long\n",
    "\n",
    "def counts_by_cue(long_df):\n",
    "    by = defaultdict(Counter)\n",
    "    for cue, resp in zip(long_df[\"cue\"], long_df[\"resp\"]):\n",
    "        by[cue][resp] += 1\n",
    "    return by\n",
    "\n",
    "def topk_overlap(h_cnt, m_cnt, k=10):\n",
    "    H = [w for w,_ in h_cnt.most_common(k)]\n",
    "    M = [w for w,_ in m_cnt.most_common(k)]\n",
    "    i = len(set(H) & set(M))\n",
    "    p = i / max(1, len(M))\n",
    "    r = i / max(1, len(H))\n",
    "    j = i / max(1, len(set(H) | set(M)))\n",
    "    f1 = 0 if (p + r) == 0 else 2 * p * r / (p + r)\n",
    "    return p, r, f1, j\n",
    "\n",
    "def dist_vectors(h_cnt, m_cnt, vocab=None, smooth=1e-9):\n",
    "    if vocab is None:\n",
    "        vocab = sorted(set(h_cnt) | set(m_cnt))\n",
    "    hv = np.array([h_cnt[w] for w in vocab], dtype=float)\n",
    "    mv = np.array([m_cnt[w] for w in vocab], dtype=float)\n",
    "    if hv.sum() == 0: hv = np.ones_like(hv)\n",
    "    if mv.sum() == 0: mv = np.ones_like(mv)\n",
    "    hp = hv / hv.sum()\n",
    "    mp = mv / mv.sum()\n",
    "    # cosine\n",
    "    cos = float(np.dot(hp, mp) / (np.linalg.norm(hp) * np.linalg.norm(mp)))\n",
    "    # JS divergence (base 2)\n",
    "    M = 0.5 * (hp + mp)\n",
    "    def _kl(p, q):\n",
    "        p = np.clip(p, smooth, 1.0)\n",
    "        q = np.clip(q, smooth, 1.0)\n",
    "        return float(np.sum(p * np.log2(p / q)))\n",
    "    jsd = 0.5 * _kl(hp, M) + 0.5 * _kl(mp, M)\n",
    "    # Entropies\n",
    "    def H(p):\n",
    "        p = np.clip(p, smooth, 1.0)\n",
    "        return float(-np.sum(p * np.log2(p)))\n",
    "    return cos, jsd, H(hp), H(mp), vocab, hp, mp\n",
    "\n",
    "def spearman_on_union(h_cnt, m_cnt):\n",
    "    vocab = sorted(set(h_cnt) | set(m_cnt))\n",
    "    if len(vocab) <= 1: return np.nan\n",
    "    hr = pd.Series({w: h_cnt[w] for w in vocab}).rank(method=\"average\")\n",
    "    mr = pd.Series({w: m_cnt[w] for w in vocab}).rank(method=\"average\")\n",
    "    corr = float(np.corrcoef(hr.values, mr.values)[0, 1])\n",
    "    return corr\n",
    "\n",
    "def hit_at_3_per_row(model_df, human_bycue):\n",
    "    df = model_df[[\"cue\",\"r1\",\"r2\",\"r3\"]].copy()\n",
    "    for c in [\"cue\",\"r1\",\"r2\",\"r3\"]:\n",
    "        df[c] = df[c].map(_norm)\n",
    "    hits = []\n",
    "    for _, row in df.iterrows():\n",
    "        cue = row[\"cue\"]\n",
    "        human_set = set(human_bycue[cue].keys())\n",
    "        trip = {row[\"r1\"], row[\"r2\"], row[\"r3\"]} - {\"\"}\n",
    "        hits.append(1 if len(trip & human_set) > 0 else 0)\n",
    "    return float(np.mean(hits))\n",
    "\n",
    "def jaccard_all_unique(h_cnt, m_cnt):\n",
    "    H = set(h_cnt.keys())\n",
    "    M = set(m_cnt.keys())\n",
    "    if not H and not M: return np.nan\n",
    "    return len(H & M) / max(1, len(H | M))\n",
    "\n",
    "def bootstrap_ci(values, n_boot=2000, alpha=0.05, rng=np.random.default_rng(0)):\n",
    "    vals = np.asarray(values, dtype=float)\n",
    "    vals = vals[~np.isnan(vals)]\n",
    "    n = len(vals)\n",
    "    if n == 0: return (np.nan, np.nan, np.nan)\n",
    "    boots = [np.mean(vals[rng.integers(0, n, n)]) for _ in range(n_boot)]\n",
    "    lo, hi = np.quantile(boots, [alpha/2, 1 - alpha/2])\n",
    "    return float(np.mean(vals)), float(lo), float(hi)\n",
    "\n",
    "# ---------- load (Excel) ----------\n",
    "hum  = pd.read_excel(HUM_PATH)\n",
    "ft   = pd.read_excel(FT_PATH)\n",
    "base = pd.read_excel(BASE_PATH)\n",
    "\n",
    "# keep the 4 columns; normalize column names\n",
    "ft   = ft[[\"cue\",\"R1\",\"R2\",\"R3\"]].rename(columns=str.lower)\n",
    "base = base[[\"cue\",\"R1\",\"R2\",\"R3\"]].rename(columns=str.lower)\n",
    "\n",
    "# long forms & counts\n",
    "hum_long  = melt_triplets(hum)\n",
    "ft_long   = melt_triplets(ft)\n",
    "base_long = melt_triplets(base)\n",
    "\n",
    "hum_by  = counts_by_cue(hum_long)\n",
    "ft_by   = counts_by_cue(ft_long)\n",
    "base_by = counts_by_cue(base_long)\n",
    "\n",
    "cues = sorted(set(hum_by) & set(ft_by) & set(base_by))\n",
    "\n",
    "# ---------- per-cue metrics ----------\n",
    "rows = []\n",
    "for cue in cues:\n",
    "    h = hum_by[cue]\n",
    "    f = ft_by[cue]\n",
    "    b = base_by[cue]\n",
    "\n",
    "    # Top-K overlap (K=10 by default)\n",
    "    Pft, Rft, Fft, Jft = topk_overlap(h, f, k=10)\n",
    "    Pba, Rba, Fba, Jba = topk_overlap(h, b, k=10)\n",
    "\n",
    "    # Full-distribution similarities\n",
    "    cos_f, jsd_f, Hh, Hf, _, _, _ = dist_vectors(h, f)\n",
    "    cos_b, jsd_b, _,  Hb, _, _, _ = dist_vectors(h, b)\n",
    "\n",
    "    # Rank agreement\n",
    "    spr_f = spearman_on_union(h, f)\n",
    "    spr_b = spearman_on_union(h, b)\n",
    "\n",
    "    # Jaccard on ALL unique responses (set overlap)\n",
    "    Jall_ft = jaccard_all_unique(h, f)\n",
    "    Jall_ba = jaccard_all_unique(h, b)\n",
    "\n",
    "    rows.append({\n",
    "        \"cue\": cue,\n",
    "        # Top-K (K=10)\n",
    "        \"P@10_ft\": Pft, \"R@10_ft\": Rft, \"F1@10_ft\": Fft, \"Jaccard@10_ft\": Jft,\n",
    "        \"P@10_base\": Pba, \"R@10_base\": Rba, \"F1@10_base\": Fba, \"Jaccard@10_base\": Jba,\n",
    "        # Full distributions\n",
    "        \"cosine_ft\": cos_f, \"cosine_base\": cos_b,\n",
    "        \"JSD_ft\": jsd_f, \"JSD_base\": jsd_b,\n",
    "        \"spearman_ft\": spr_f, \"spearman_base\": spr_b,\n",
    "        \"H_human\": Hh, \"H_ft\": Hf, \"H_base\": Hb,\n",
    "        # Jaccard over ALL unique responses\n",
    "        \"Jaccard_all_ft\": Jall_ft,\n",
    "        \"Jaccard_all_base\": Jall_ba,\n",
    "    })\n",
    "\n",
    "percue = pd.DataFrame(rows)\n",
    "\n",
    "# row-level Hit@3\n",
    "ft_hit3   = hit_at_3_per_row(ft,   hum_by)\n",
    "base_hit3 = hit_at_3_per_row(base, hum_by)\n",
    "\n",
    "# ---------- headline (macro) metrics with bootstrap CIs ----------\n",
    "def macro(col_ft, col_ba):\n",
    "    x = percue[col_ft].values\n",
    "    y = percue[col_ba].values\n",
    "    m_ft, lo_ft, hi_ft = bootstrap_ci(x)\n",
    "    m_ba, lo_ba, hi_ba = bootstrap_ci(y)\n",
    "    return {\n",
    "        \"ft_mean\": m_ft, \"ft_CI\": (lo_ft, hi_ft),\n",
    "        \"base_mean\": m_ba, \"base_CI\": (lo_ba, hi_ba),\n",
    "        \"delta\": m_ft - m_ba\n",
    "    }\n",
    "\n",
    "# Dynamic summary: include ALL *_ft/*_base pairs and human-only macros\n",
    "pretty_name = {\"JSD\": \"JSD (lower better)\"}\n",
    "\n",
    "def _prettify(metric_key: str) -> str:\n",
    "    # metric_key is without suffix (e.g., \"P@10\", \"cosine\", \"JSD\", \"spearman\", \"H\", \"Jaccard_all\")\n",
    "    base = metric_key\n",
    "    if base.lower() == \"cosine\":   base = \"Cosine\"\n",
    "    if base.lower() == \"spearman\": base = \"Spearman\"\n",
    "    # Entropy\n",
    "    if base in {\"H\"} or base.startswith(\"H_\") or base == \"H\":\n",
    "        return \"Entropy\"\n",
    "    # Special label for JSD\n",
    "    if base == \"JSD\":\n",
    "        return pretty_name[\"JSD\"]\n",
    "    return base\n",
    "\n",
    "summary = {\n",
    "    \"Hit@3 (row-level)\": {\"ft\": ft_hit3, \"base\": base_hit3, \"delta\": ft_hit3 - base_hit3}\n",
    "}\n",
    "\n",
    "# add every *_ft / *_base pair automatically\n",
    "for col in percue.columns:\n",
    "    if col.endswith(\"_ft\"):\n",
    "        base_col = col[:-3] + \"_base\"\n",
    "        if base_col in percue.columns:\n",
    "            metric_key = col[:-3]  # strip \"_ft\"\n",
    "            label = _prettify(metric_key)\n",
    "            summary[label] = macro(col, base_col)\n",
    "\n",
    "# add macro means for any human-only columns (e.g., H_human)\n",
    "human_only = {}\n",
    "for col in percue.columns:\n",
    "    if col.endswith(\"_human\"):\n",
    "        m, lo, hi = bootstrap_ci(percue[col].values)\n",
    "        # label human metrics nicely\n",
    "        if col.startswith(\"H_\"):\n",
    "            lab = \"Entropy (human)\"\n",
    "        else:\n",
    "            lab = f\"{col.replace('_', ' ').title()}\"\n",
    "        human_only[lab] = {\"mean\": m, \"CI\": (lo, hi)}\n",
    "\n",
    "if human_only:\n",
    "    summary[\"Human-only macro means\"] = human_only\n",
    "\n",
    "# ---------- print & save ----------\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(\"=== Headline metrics (FT vs Base vs Human) ===\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# save detailed per-cue table (Excel)\n",
    "OUT = os.path.join(BASE_PATH, r\"data/fine_tuned_and_base_models_comparisons/free_associations_task/eval_full_val_ft_vs_base_vs_human_per_cue.xlsx\")\n",
    "percue.sort_values(\"cue\").to_excel(OUT, index=False)\n",
    "print(f\"\\nPer-cue metrics saved to: {OUT}\")\n",
    "\n",
    "# quick peek at top-10 lists for some cues (optional)\n",
    "def top_list(cnt, k=10): return [w for w,_ in cnt.most_common(k)]\n",
    "peek = []\n",
    "for cue in cues[:25]:\n",
    "    peek.append({\n",
    "        \"cue\": cue,\n",
    "        \"top10_human\": \", \".join(top_list(hum_by[cue], 10)),\n",
    "        \"top10_ft\":    \", \".join(top_list(ft_by[cue], 10)),\n",
    "        \"top10_base\":  \", \".join(top_list(base_by[cue], 10)),\n",
    "    })\n",
    "print(\"\\nSample of top-10 lists (first 10 cues):\")\n",
    "print(pd.DataFrame(peek).head(10).to_string(index=False))\n",
    "\n",
    "# optional: also show the first few per-cue metric rows\n",
    "print(\"\\n=== All metrics per cue (first 10 cues) ===\")\n",
    "print(percue.head(10).to_string(index=False))\n",
    "\n",
    "# save summary JSON (unchanged)\n",
    "SUMMARY_OUT = os.path.join(BASE_PATH, r\"data/models_predictions_associations_task/full_llama3_8b_system_prompt_lora_SFT_SWOW_tgt_qkvo_tr7194c_val899c_r16_a32_do0p1_lr0.0001_bs16_ga4/eval_full_val_ft_vs_base_vs_human_summary.json\")\n",
    "with open(SUMMARY_OUT, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Summary JSON saved to: {SUMMARY_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpErQrnTNx13"
   },
   "outputs": [],
   "source": [
    "# Disconnect the runtime\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc5PH8xCNxzg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or3juAViOdB5"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 🧪  Generate 80× responses per cue on validation split\n",
    "#      • fine-tuned model\n",
    "#      • base model\n",
    "#      + save raw outputs, conversation history, and prompts\n",
    "#      + batched via num_return_sequences with OOM fallback\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "import os, gc, re, time, torch, pandas as pd, math\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# ---------- PATHS -------------------------------------------------\n",
    "VAL_XLSX_PATH  = os.path.join(BASE_PATH, r\"data/final_processed_SWOW_data/val.xlsx\")\n",
    "\n",
    "# TODO: update FINETUNED_MODEL to your current run's merged or adapter path\n",
    "FINETUNED_MODEL = os.path.join(BASE_PATH, r\"data/fine_tuned_and_base_models_comparisons/free_associations_task/full_llama3_8b_system_prompt_lora_SFT_SWOW_tgt_qkvo_tr7194c_val899c_r16_a32_do0p1_lr0.0001_bs16_ga4/merged_model\")\n",
    "BASE_MODEL      = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "SAVE_DIR_FT   = os.path.join(BASE_PATH, r\"data/models_predictions_associations_task/full_llama3_8b_system_prompt_lora_SFT_SWOW_tgt_qkvo_tr7194c_val899c_r16_a32_do0p1_lr0.0001_bs16_ga4\")\n",
    "os.makedirs(SAVE_DIR_FT, exist_ok=True)\n",
    "\n",
    "SAVE_DIR_BASE = os.path.join(BASE_PATH, r\"data/models_predictions_associations_task/llama3_8b_base_model\")\n",
    "os.makedirs(SAVE_DIR_BASE, exist_ok=True)\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# ---------- Precision / quant (use the SAME for both models) --------\n",
    "USE_4BIT = False  # set True if you prefer 4-bit generation for both\n",
    "if USE_4BIT:\n",
    "    load_kwargs = dict(\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,  # A100-friendly\n",
    "        ),\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "else:\n",
    "    load_kwargs = dict(\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,  # A100-friendly; consistent across models\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# ---------- Sampling knobs (unchanged except batching) ---------------\n",
    "NUM_SAMPLES_PER_CUE = 80\n",
    "TRY_AT_ONCE         = 80   # first attempt per call; auto-shrinks on OOM\n",
    "MAX_NEW_TOKENS      = 50\n",
    "TEMPERATURE         = 0.7\n",
    "TOP_P               = 0.9\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "Task:\n",
    " - You will be provided with an input word: write the first 3 words you associate to it separated by a comma.\n",
    " - No additional output text is allowed.\n",
    "\n",
    "Constraints:\n",
    " - no carriage return characters are allowed in the answers.\n",
    " - answers should be as short as possible.\n",
    "\n",
    "Example:\n",
    " Input: sea\n",
    " Output: water, beach, sun\"\"\"\n",
    "\n",
    "def build_prompt_text(tokenizer, cue: str) -> str:\n",
    "    \"\"\"Render messages with the official Llama-3 chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": str(cue).strip()},\n",
    "    ]\n",
    "    # This appends the assistant header so generation starts in assistant turn\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def build_history(cue: str, assistant_text: str) -> str:\n",
    "    \"\"\"Readable transcript (system → user → assistant).\"\"\"\n",
    "    return (\n",
    "        \"<system>\\n\"   + SYSTEM_PROMPT.strip() + \"\\n</system>\\n\"\n",
    "        \"<user>\\n\"     + str(cue).strip()      + \"\\n</user>\\n\"\n",
    "        \"<assistant>\\n\"+ (assistant_text or \"\").strip() + \"\\n</assistant>\"\n",
    "    )\n",
    "\n",
    "def extract_responses(text: str, n=3):\n",
    "    \"\"\"\n",
    "    Extract up to n words from a comma-separated generation.\n",
    "    We *don’t* expect 'Answer:' anymore. Normalize lightly.\n",
    "    \"\"\"\n",
    "    # Keep letters, digits, spaces, hyphens, and commas; lowercase\n",
    "    clean = re.sub(r\"[^\\w,\\- ]\", \"\", text.lower())\n",
    "    # robust split for \",\", \" ,\", \", \" etc.\n",
    "    words = [w.strip() for w in re.split(r\"\\s*,\\s*\", clean) if w.strip()]\n",
    "    return (words + [\"\", \"\", \"\"])[:n]\n",
    "\n",
    "def _generate_many_for_cue(model, tok, cue: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate NUM_SAMPLES_PER_CUE samples for one cue using num_return_sequences,\n",
    "    automatically backing off the chunk size if CUDA runs out of memory.\n",
    "    \"\"\"\n",
    "    prompt_text = build_prompt_text(tok, cue)\n",
    "    prompt_ids  = tok(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    recs = []\n",
    "    remaining   = NUM_SAMPLES_PER_CUE\n",
    "    try_at_once = TRY_AT_ONCE\n",
    "\n",
    "    while remaining > 0:\n",
    "        cur_n = min(try_at_once, remaining)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **prompt_ids,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    do_sample=True, temperature=TEMPERATURE, top_p=TOP_P,\n",
    "                    pad_token_id=tok.eos_token_id,\n",
    "                    eos_token_id=tok.eos_token_id,  # tokenizer maps EOT properly\n",
    "                    num_return_sequences=cur_n,      # ← key: many samples at once\n",
    "                )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # Back off and try a smaller chunk\n",
    "            torch.cuda.empty_cache()\n",
    "            try_at_once = max(1, try_at_once // 2)\n",
    "            # if we already tried size 1 and failed, re-raise\n",
    "            if cur_n == 1 and try_at_once == 1:\n",
    "                raise\n",
    "            continue\n",
    "\n",
    "        # out has shape [cur_n, seq_len]; decode each\n",
    "        prompt_len = prompt_ids[\"input_ids\"].shape[1]\n",
    "        for i in range(out.size(0)):\n",
    "            gen_only = out[i, prompt_len:]\n",
    "            raw = tok.decode(gen_only, skip_special_tokens=True)\n",
    "\n",
    "            r1, r2, r3 = extract_responses(raw)\n",
    "            recs.append(dict(\n",
    "                cue=cue,\n",
    "                R1=r1, R2=r2, R3=r3,\n",
    "                raw_output=raw,\n",
    "                history=build_history(cue, raw),\n",
    "                system_prompt=SYSTEM_PROMPT.strip(),\n",
    "                user_prompt=str(cue).strip(),\n",
    "            ))\n",
    "\n",
    "        remaining -= cur_n\n",
    "\n",
    "    return recs\n",
    "\n",
    "def generate_dataset(model_id_or_path: str, tag: str):\n",
    "    \"\"\"Return list[dict]: {cue, R1, R2, R3, raw_output, history, system_prompt, user_prompt} for the val cues.\"\"\"\n",
    "    print(f\"\\n🔹 Loading {tag} model …\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id_or_path, use_fast=True)\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id_or_path, **load_kwargs)\n",
    "    model.eval()\n",
    "\n",
    "    # ---------- read validation cues from Excel ----------\n",
    "    val_df = pd.read_excel(VAL_XLSX_PATH)\n",
    "    cues = val_df[\"cue\"].dropna().astype(str).str.strip().unique()\n",
    "    recs = []\n",
    "\n",
    "    for cue in tqdm(cues, desc=f\"{tag} {NUM_SAMPLES_PER_CUE}× per cue\"):\n",
    "        recs.extend(_generate_many_for_cue(model, tok, cue))\n",
    "        torch.cuda.empty_cache()  # light hygiene between cues\n",
    "\n",
    "    # -------- cleanup to free VRAM --------\n",
    "    del model; del tok; gc.collect(); torch.cuda.empty_cache()\n",
    "    return recs\n",
    "\n",
    "# ---------- Fine-tuned model -----------------------------------\n",
    "t0 = time.time()\n",
    "ft_rows = generate_dataset(FINETUNED_MODEL, \"Finetuned\")\n",
    "ft_tag  = os.path.basename(os.path.normpath(FINETUNED_MODEL))  # folder name like 'merged_model' or run dir\n",
    "ft_path = os.path.join(SAVE_DIR_FT, f\"full_val_ft_{ft_tag}_predictions.xlsx\")\n",
    "pd.DataFrame(ft_rows).to_excel(ft_path, index=False)  # ← Excel\n",
    "print(f\"Fine-tuned predictions saved → {ft_path} ({len(ft_rows):,} rows)\")\n",
    "print(f\"{time.time()-t0:.1f}s elapsed\")\n",
    "\n",
    "# ---------- Base model -----------------------------------------\n",
    "t1 = time.time()\n",
    "base_rows = generate_dataset(BASE_MODEL, \"Base\")\n",
    "base_path = os.path.join(SAVE_DIR_BASE, \"llama3_8b_full_val_base_predictions.xlsx\")\n",
    "pd.DataFrame(base_rows).to_excel(base_path, index=False)  # ← Excel\n",
    "print(f\"Base-model predictions saved → {base_path} ({len(base_rows):,} rows)\")\n",
    "print(f\"{time.time()-t1:.1f}s elapsed (second run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-ZfrIo-FKp5"
   },
   "outputs": [],
   "source": [
    "# Disconnect the runtime\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTc8WaDOgmma"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz1SgWy5gmjc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jz4nen3FhVih"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM1EHisv0ey6E3ickd9URjr",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1dqCZ8RYPI9JHnNe7cYbgTQvDa1pr7aDz",
     "timestamp": 1754852023215
    },
    {
     "file_id": "1_4krYAQIpm-TCyf8s6hBcwsFVxFQMmgn",
     "timestamp": 1754828998873
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
