{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZS9oYu43-zv"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ðŸ”§ Setup: Install Packages\n",
    "# ===============================\n",
    "!pip install -q \\\n",
    "  \"transformers>=4.41,<5\" \\\n",
    "  \"datasets==2.19.1\" \\\n",
    "  \"peft==0.10.0\" \\\n",
    "  \"accelerate>=0.34.2\" \\\n",
    "  \"bitsandbytes>=0.43.3\" \\\n",
    "  \"scikit-learn\" \\\n",
    "  \"openpyxl\" \\\n",
    "  \"pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RIxSW6t1JiL"
   },
   "outputs": [],
   "source": [
    "import torch, sys, subprocess\n",
    "mm = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "triton_by_torch = {\"2.5\":\"3.2.0\",\"2.4\":\"3.0.0\",\"2.3\":\"2.3.1\",\"2.2\":\"2.2.0\"}\n",
    "target = triton_by_torch.get(mm, \"3.2.0\")\n",
    "print(f\"Torch {torch.__version__} â†’ Installing Triton {target}\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"triton=={target}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8tfqh_s4DkC"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, random, torch, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer, set_seed\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --------------- Hugging Face token ---------------\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "login(os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# --------------- Reproducibility ---------------\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMfTIXz64DeY"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Build chat records for SFT\n",
    "#         â€¢ system    - task + constraints + example\n",
    "#         â€¢ user      - cue word only\n",
    "#         â€¢ assistant - gold 3-word answer\n",
    "#         â€¢ rendered with tokenizer.apply_chat_template(...)\n",
    "# ============================================\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from textwrap import dedent\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ---------- Model (for chat template rendering) ----------\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # explicit pad for Llama-3\n",
    "tokenizer.padding_side = \"right\"     # training\n",
    "tokenizer.truncation_side = \"left\"   # keep the assistant answer in-frame\n",
    "\n",
    "# ---------- system prompt ----------\n",
    "SYSTEM_PROMPT = dedent(\"\"\"\\\n",
    "Task:\n",
    " - You will be provided with an input word: write the first 3 words you associate to it separated by a comma.\n",
    " - No additional output text is allowed.\n",
    "\n",
    "Constraints:\n",
    " - no carriage return characters are allowed in the answers.\n",
    " - answers should be as short as possible.\n",
    "\n",
    "Example:\n",
    " Input: sea\n",
    " Output: water, beach, sun\"\"\").strip()\n",
    "\n",
    "def build_chat(cue: str, responses: list[str]) -> str:\n",
    "    assistant_txt = \", \".join(responses)  # space after each comma\n",
    "    messages = [\n",
    "        {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",      \"content\": cue},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_txt},\n",
    "    ]\n",
    "    # Render full conversation (with assistant content) as a string\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=False, tokenize=False\n",
    "    )\n",
    "\n",
    "def df_to_records(df: pd.DataFrame):\n",
    "    recs = []\n",
    "    for _, row in df.iterrows():\n",
    "        cue  = str(row[\"cue\"]).strip()\n",
    "        resp = [str(row[\"R1\"]).strip(), str(row[\"R2\"]).strip(), str(row[\"R3\"]).strip()]\n",
    "        recs.append(dict(cue=cue, text=build_chat(cue, resp)))\n",
    "    return recs\n",
    "\n",
    "# ---------- Load data ----------\n",
    "BASE_PATH = r\"/content/drive/My Drive/associations-ANLP\"\n",
    "DATA_DIR = os.path.join(BASE_PATH, r\"data/final_processed_SWOW_data\")\n",
    "train_df = pd.read_excel(os.path.join(DATA_DIR, \"train.xlsx\"))\n",
    "val_df   = pd.read_excel(os.path.join(DATA_DIR, \"val.xlsx\"))\n",
    "test_df  = pd.read_excel(os.path.join(DATA_DIR, \"test.xlsx\"))\n",
    "\n",
    "# ---------- Generate records ----------\n",
    "train_recs = df_to_records(train_df)\n",
    "val_recs   = df_to_records(val_df)\n",
    "test_recs  = df_to_records(test_df)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Wrap in DatasetDict ----------\n",
    "data = DatasetDict({\n",
    "    \"train\":      Dataset.from_list(train_recs),\n",
    "    \"validation\": Dataset.from_list(val_recs),\n",
    "    \"test\":       Dataset.from_list(test_recs),\n",
    "})\n",
    "\n",
    "print({k: len(v) for k, v in data.items()})\n",
    "display(data[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9DUJbF84Dbu"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Tokenise pre-rendered chat text (already templated)\n",
    "#   â€¢ input: columns {cue, text} where 'text' is chat-template rendered\n",
    "#   â€¢ output: input_ids & attention_mask (keep \"text\" for debugging/export)\n",
    "# =========================================================\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd, os\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token   # Llama-3 needs explicit pad token\n",
    "tokenizer.padding_side = \"right\"     # training\n",
    "tokenizer.truncation_side = \"left\"   # keep the assistant answer in-frame\n",
    "\n",
    "\n",
    "# We already rendered via apply_chat_template in Cell 1\n",
    "assert \"text\" in data[\"train\"].column_names, \"Expected a 'text' column with rendered chat.\"\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=False, truncation=True, max_length=1024)\n",
    "\n",
    "tokenised = data.map(tok_fn, batched=True)\n",
    "\n",
    "# Keep only what we need (+ 'text' if you want to export/inspect)\n",
    "tokenised = tokenised.remove_columns(\n",
    "    [c for c in tokenised[\"train\"].column_names if c not in {\"input_ids\",\"attention_mask\",\"text\"}]\n",
    ")\n",
    "\n",
    "print(\"Example token IDs:\", tokenised[\"train\"][0][\"input_ids\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUptkeoo4DZX"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LoRA-prepared quantised model (q,v; r8; dropout 0.10)\n",
    "# ============================================\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import torch, os\n",
    "\n",
    "# ---- Hyperparameters ----\n",
    "LORA_R, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.10\n",
    "NUM_EPOCHS      = 1\n",
    "BATCH_SIZE      = 16\n",
    "GRAD_ACC_STEPS  = 4\n",
    "LR              = 1e-4\n",
    "\n",
    "# ---- LoRA target modules (q,k,v,o) ----\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# ---- Count unique cues for run name (requires 'data' from earlier cell) ----\n",
    "try:\n",
    "    N_TRAIN_CUES = len(set(data[\"train\"][\"cue\"]))\n",
    "    N_VAL_CUES   = len(set(data[\"validation\"][\"cue\"]))\n",
    "except Exception:\n",
    "    N_TRAIN_CUES = -1\n",
    "    N_VAL_CUES   = -1\n",
    "\n",
    "# Build a compact target tag for the run name\n",
    "_letter_map = {\"q_proj\": \"q\", \"k_proj\": \"k\", \"v_proj\": \"v\", \"o_proj\": \"o\",\n",
    "               \"up_proj\": \"up\", \"down_proj\": \"dn\", \"gate_proj\": \"gt\"}\n",
    "_order = {\"q\":0,\"k\":1,\"v\":2,\"o\":3}\n",
    "_target_letters = sorted([_letter_map.get(m, m) for m in TARGET_MODULES],\n",
    "                         key=lambda x: _order.get(x, 99))\n",
    "TARGET_TAG = \"tgt_\" + \"\".join(_target_letters)\n",
    "\n",
    "RUN_NAME = (\n",
    "    f\"full_llama3_8b_system_prompt_lora_SFT_SWOW_{TARGET_TAG}\"\n",
    "    f\"_tr{N_TRAIN_CUES}c_val{N_VAL_CUES}c\"\n",
    "    f\"_r{LORA_R}_a{LORA_ALPHA}_do{str(LORA_DROPOUT).replace('.','p')}\"\n",
    "    f\"_lr{LR:g}_bs{BATCH_SIZE}_ga{GRAD_ACC_STEPS}\"\n",
    ")\n",
    "\n",
    "BASE_PATH = r\"/content/drive/My Drive/associations-ANLP\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, RUN_NAME)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"Output dir:\", OUTPUT_DIR)\n",
    "\n",
    "# ---- 4-bit quant loader (bf16 compute on A100) ----\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", quantization_config=bnb_cfg\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "# (optional, helps memory)\n",
    "model.gradient_checkpointing_enable()\n",
    "try:\n",
    "    model.enable_input_require_grads()\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ---- SFT collator with LABEL MASKING ----\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "class MaskedSFTCollator:\n",
    "    def __init__(self, tok):\n",
    "        self.tok = tok\n",
    "        self.assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        features = [{k: v for k, v in f.items()\n",
    "                     if k in (\"input_ids\", \"attention_mask\")} for f in features]\n",
    "        batch = self.tok.pad(features, padding=\"longest\", return_tensors=\"pt\")\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # mask PAD tokens so they never contribute to loss\n",
    "        am = batch[\"attention_mask\"]              # (B, T)\n",
    "        labels[am == 0] = -100\n",
    "\n",
    "        # mask everything up to (and including) the assistant header\n",
    "        for i, ids in enumerate(input_ids):\n",
    "            txt = self.tok.decode(ids, skip_special_tokens=False)\n",
    "            pos = txt.find(self.assistant_header)\n",
    "            if pos == -1:\n",
    "                labels[i].fill_(-100)\n",
    "                continue\n",
    "            prefix = txt[: pos + len(self.assistant_header)]\n",
    "            cutoff = len(self.tok(prefix, add_special_tokens=False)[\"input_ids\"])\n",
    "            labels[i, :cutoff] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "collator = MaskedSFTCollator(tokenizer)\n",
    "\n",
    "# Quick sanity check once\n",
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(tokenised[\"train\"], batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "batch = next(iter(dl))\n",
    "assert (batch[\"labels\"] != -100).any(), \"All labels masked â†’ assistant header not found\"\n",
    "print(f\"Masking sanity check passed | RUN_NAME={RUN_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rx6UNAkL4DWu"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TrainingArguments & Trainer\n",
    "# ============================================\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.trainer_callback import EarlyStoppingCallback  # optional\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "\n",
    "    # Use the QLoRA-friendly optimizer\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    "    # Stability & regularization\n",
    "    weight_decay=0.05,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Schedule\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "\n",
    "    # Precision\n",
    "    bf16=True, fp16=False,\n",
    "    # Also expose GC flag here (in addition to model.gradient_checkpointing_enable())\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Logging / eval / save cadence\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=800,                 # was 400\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=800,                 # was 400\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    # Throughput niceties\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised[\"train\"],\n",
    "    eval_dataset=tokenised[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # <-- enable only if epochs>1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3FDwlrP4DUT"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Train (resume aware)\n",
    "# ============================================\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import time, numpy as _np, torch.serialization as _ser, inspect, numpy as np\n",
    "\n",
    "if hasattr(_ser, \"_user_allowed_globals\"):\n",
    "    _ser._user_allowed_globals.clear()\n",
    "_ser.add_safe_globals([\n",
    "    _np.core.multiarray._reconstruct, _np.ndarray, _np.dtype,\n",
    "    *[cls for _, cls in inspect.getmembers(_np.dtypes, inspect.isclass) if issubclass(cls, _np.dtype)]\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "ckpt = get_last_checkpoint(OUTPUT_DIR)\n",
    "if ckpt is None:\n",
    "    print(\"No checkpoint â€“ starting fresh\")\n",
    "    trainer.train()\n",
    "else:\n",
    "    print(f\"Resuming from {ckpt}\")\n",
    "    trainer.train(resume_from_checkpoint=ckpt)\n",
    "\n",
    "print(f\"Elapsed (min): {(time.time()-start)/60:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NT4GKeqb4DLa"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Save LoRA adapter + merged model\n",
    "# ============================================\n",
    "adapter_path = os.path.join(OUTPUT_DIR, \"lora_adapter\")\n",
    "merged_path  = os.path.join(OUTPUT_DIR, \"merged_model\")\n",
    "os.makedirs(adapter_path, exist_ok=True); os.makedirs(merged_path, exist_ok=True)\n",
    "\n",
    "print(\" Saving raw LoRA adapter â€¦\")\n",
    "trainer.model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(\" Merging LoRA weights into base model â€¦\")\n",
    "with torch.no_grad():\n",
    "    merged_lm = trainer.model.merge_and_unload()\n",
    "\n",
    "print(\" Saving merged model â€¦\")\n",
    "merged_lm.save_pretrained(merged_path, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(\" Finished!\\n â€¢ Adapter â†’\", adapter_path, \"\\n â€¢ Merged model â†’\", merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8JJ9rxU4pX4"
   },
   "outputs": [],
   "source": [
    "# Disconnect the runtime\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNIY7RAE6j64iZtdeIM6cYV",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1dqCZ8RYPI9JHnNe7cYbgTQvDa1pr7aDz",
     "timestamp": 1754852023215
    },
    {
     "file_id": "1_4krYAQIpm-TCyf8s6hBcwsFVxFQMmgn",
     "timestamp": 1754828998873
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
